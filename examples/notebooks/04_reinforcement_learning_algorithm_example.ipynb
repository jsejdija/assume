{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JeBorbE6FYr"
   },
   "source": [
    "# 4. Reinforcement learning tutorial\n",
    "\n",
    "This tutorial will introduce users into the MATD3 implementation in ASSUME and hence how we use reinforcement leanring (RL). The main objective of this tutorial is to ensure participants grasp the steps required to equip ASSUME with a RL alogorithm. It ,therefore, start one level deeper, than the RL_application example and the knowledge from this tutorial i not required, if the already perconfigured algorithm in Assume should be used. The algorithm explained here is usable as a plug and play solution in the framework. The following coding tasks will highlight the key in the algorithm class and will explain the interactions with the learning role and other classes along the way. \n",
    "\n",
    "The outline of this tutorial is as follows. We will start with an introduction to the changed simualtion flow when we use reinforcement learning (1. From one simulation year to learning episodes). If you need a refresher on RL in general, please visit our readthedocs (https://assume.readthedocs.io/en/latest/). Afterwards, we dive into the tasks and reason behind a learning role (2. What role has a learning role) and then dive into the characteristics of the algorithm (3. The MATD3).\n",
    "\n",
    "**As a whole, this tutorial covers the following coding tasks:**\n",
    "\n",
    "1. xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Assume\n",
    "\n",
    "Frist we need to install Assume in this Colab. Here we just install the ASSUME core package via pip. In general the instructions for an installation can be found here: https://assume.readthedocs.io/en/latest/installation.html. All the required steps are executed here and since we are working in colab the generation of a venv is not necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0DaRwFA7VgW",
    "outputId": "5655adad-5b7a-4fe3-9067-6b502a06136b",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install assume-framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIw_QIE3pY34"
   },
   "source": [
    "And easy like this we have ASSUME installed. Now we can let it run. Please note though that we cannot use the functionalities tied to docker and, hence, cannot access the predefined dashboards in colab. For this please install docker and ASSUME on your personal machine.\n",
    "\n",
    "Further we would like to access the predefined scenarios in ASSUME which are stored on the git repository. Hence, we clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5hB0uDisSsg",
    "outputId": "1241881f-e090-4f26-9b02-560adfcb3a3e",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/assume-framework/assume.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg7DyNjLuvSb"
   },
   "source": [
    "**Let the magic happen.** Now you can run your first ever simulation in ASSUME. The following code navigates to the respective assume folder and starts the simulation example example_01b using the local database here in colab.\n",
    "\n",
    "When running locally, you can also just run `assume -s example_01b -db \"sqlite:///./examples/local_db/assume_db_example_01b.db\"` in a shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eVM60Qx8SC0",
    "outputId": "20434515-6e65-4d34-d44d-8c4529a46ece",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!cd assume && assume -s example_01b -db \"sqlite:///./examples/local_db/assume_db_example_01b.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj2C4ElILNNv"
   },
   "source": [
    "## 1. From one simulation year to learning episodes\n",
    "\n",
    "In a normal simulation wihtout reinforcement learning, we only run the time horizon of the simulation once. For RL the agents need to learn their strategy based on interactions. For that to work an RL agent has to see a situation, aka a simulation hour, multiple times, and hence we need to run the entire silumation hoirzon multiple times as well.   \n",
    "\n",
    "To enable this we define a run learning function that will be called if the simulation is started and we defined in our config that we want to activate learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMyZhaNM7NRP"
   },
   "source": [
    "**But first some imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoWI_agIJOE4",
    "outputId": "9b40e670-bfef-4560-d6e8-61a1b29d1975",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# install jdc for some in line magic,\n",
    "# that allows us defining functions of classes across different cells\n",
    "\n",
    "!pip install jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xUsbeZdPJ_2Q"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import dateutil.rrule as rr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from assume.common.base import LearningConfig\n",
    "from assume.common.exceptions import AssumeException\n",
    "from assume.common.forecasts import CsvForecaster, Forecaster\n",
    "from assume.common.market_objects import MarketConfig, MarketProduct\n",
    "from assume.world import World\n",
    "from assume.scenario.loader_csv import load_scenario_folder\n",
    "from assume.reinforcement_learning.buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UXYSesx4Ifp5"
   },
   "outputs": [],
   "source": [
    "def run_learning(\n",
    "    world: World, inputs_path: str, scenario: str, study_case: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train Deep Reinforcement Learning (DRL) agents to act in a simulated market environment.\n",
    "\n",
    "    This function runs multiple episodes of simulation to train DRL agents, performs evaluation, and saves the best runs. It maintains the buffer and learned agents in memory to avoid resetting them with each new run.\n",
    "\n",
    "    Args:\n",
    "        world (World): An instance of the World class representing the simulation environment.\n",
    "        inputs_path (str): The path to the folder containing input files necessary for the simulation.\n",
    "        scenario (str): The name of the scenario for the simulation.\n",
    "        study_case (str): The specific study case for the simulation.\n",
    "\n",
    "    Note:\n",
    "        - The function uses a ReplayBuffer to store experiences for training the DRL agents.\n",
    "        - It iterates through training episodes, updating the agents and evaluating their performance at regular intervals.\n",
    "        - Initial exploration is active at the beginning and is disabled after a certain number of episodes to improve the performance of DRL algorithms.\n",
    "        - Upon completion of training, the function performs an evaluation run using the best policy learned during training.\n",
    "        - The best policies are chosen based on the average reward obtained during the evaluation runs, and they are saved for future use.\n",
    "    \"\"\"\n",
    "        # remove csv path so that nothing is written while learning\n",
    "    temp_csv_path = world.export_csv_path\n",
    "    world.export_csv_path = \"\"\n",
    "    \n",
    "    initialize_buffer()\n",
    "\n",
    "    handle_storage_paths()\n",
    "\n",
    "\n",
    "    run_learning_loop()\n",
    "\n",
    "\n",
    "        # container shutdown implicitly with new initialisation\n",
    "    logger.info(\"################\")\n",
    "    logger.info(\"Training finished, Start evaluation run\")\n",
    "    world.export_csv_path = temp_csv_path\n",
    "\n",
    "    # load scenario for evaluation\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path,\n",
    "        scenario,\n",
    "        study_case,\n",
    "        perform_learning=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_buffer():\n",
    "\n",
    "    buffer = ReplayBuffer(\n",
    "        buffer_size=int(world.learning_config.get(\"replay_buffer_size\", 5e5)),\n",
    "        obs_dim=world.learning_role.obs_dim,\n",
    "        act_dim=world.learning_role.act_dim,\n",
    "        n_rl_units=len(world.learning_role.rl_strats),\n",
    "        device=world.learning_role.device,\n",
    "        float_type=world.learning_role.float_type,\n",
    "    )\n",
    "    \n",
    "    actors_and_critics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_storage_paths():\n",
    "\n",
    "\n",
    "    world.output_role.del_similar_runs()\n",
    "\n",
    "\n",
    "    save_path = world.learning_config[\"trained_policies_save_path\"]\n",
    "\n",
    "    if Path(save_path).is_dir():\n",
    "        # we are in learning mode and about to train new policies, which might overwrite existing ones\n",
    "        accept = input(\n",
    "            f\"{save_path=} exists - should we overwrite current learnings? (y/N)\"\n",
    "        )\n",
    "        if not accept.lower().startswith(\"y\"):\n",
    "            # stop here - do not start learning or save anything\n",
    "            raise AssumeException(\"don't overwrite existing strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learning_loop():\n",
    "    \n",
    "    validation_interval = min(\n",
    "        world.learning_role.training_episodes,\n",
    "        world.learning_config.get(\"validation_episodes_interval\", 5),\n",
    "    )\n",
    "\n",
    "    eval_episode = 1\n",
    "\n",
    "    for episode in tqdm(\n",
    "        range(1, world.learning_role.training_episodes + 1),\n",
    "        desc=\"Training Episodes\",\n",
    "    ):\n",
    "        # TODO normally, loading twice should not create issues, somehow a scheduling issue is raised currently\n",
    "        if episode != 1:\n",
    "            load_scenario_folder(\n",
    "                world,\n",
    "                inputs_path,\n",
    "                scenario,\n",
    "                study_case,\n",
    "                perform_learning=True,\n",
    "                episode=episode,\n",
    "            )\n",
    "\n",
    "        # give the newly created rl_agent the buffer that we stored from the beginning\n",
    "        world.learning_role.initialize_policy(actors_and_critics=actors_and_critics)\n",
    "\n",
    "        world.learning_role.buffer = buffer\n",
    "        world.learning_role.episodes_done = episode\n",
    "\n",
    "        if episode > world.learning_role.episodes_collecting_initial_experience:\n",
    "            world.learning_role.turn_off_initial_exploration()\n",
    "\n",
    "        world.run()\n",
    "\n",
    "        actors_and_critics = world.learning_role.rl_algorithm.extract_policy()\n",
    "\n",
    "        if (\n",
    "            episode % validation_interval == 0\n",
    "            and episode > world.learning_role.episodes_collecting_initial_experience\n",
    "        ):\n",
    "            # save current params in training path\n",
    "            world.learning_role.rl_algorithm.save_params(directory=save_path)\n",
    "            world.reset()\n",
    "\n",
    "            # load validation run\n",
    "            load_scenario_folder(\n",
    "                world,\n",
    "                inputs_path,\n",
    "                scenario,\n",
    "                study_case,\n",
    "                perform_learning=False,\n",
    "                perform_evaluation=True,\n",
    "                eval_episode=eval_episode,\n",
    "            )\n",
    "\n",
    "            world.run()\n",
    "\n",
    "            total_rewards = world.output_role.get_sum_reward()\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            # check reward improvement in validation run\n",
    "            # and store best run in eval folder\n",
    "            world.learning_role.compare_and_save_policies({\"avg_reward\": avg_reward})\n",
    "\n",
    "            eval_episode += 1\n",
    "        world.reset()\n",
    "\n",
    "        # in load_scenario_folder_async, we initiate new container and kill old if present\n",
    "        # as long as we do not skip setup container should be handled correctly\n",
    "        # if enough initial experience was collected according to specifications in learning config\n",
    "        # turn off initial exploration and go into full learning mode\n",
    "        if episode >= world.learning_role.episodes_collecting_initial_experience:\n",
    "            world.learning_role.turn_off_initial_exploration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UM1QPZrIdqK"
   },
   "source": [
    "## 2. What role has a learning role\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learning(Role):\n",
    "    \"\"\"\n",
    "    This class manages the learning process of reinforcement learning agents, including initializing key components such as\n",
    "    neural networks, replay buffer, and learning hyperparameters. It handles both training and evaluation modes based on\n",
    "    the provided learning configuration.\n",
    "\n",
    "    Args:\n",
    "        simulation_start (datetime.datetime): The start of the simulation.\n",
    "        simulation_end (datetime.datetime): The end of the simulation.\n",
    "        learning_config (LearningConfig): The configuration for the learning process.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_config: LearningConfig,\n",
    "        start: datetime,\n",
    "        end: datetime,\n",
    "    ):\n",
    "        self.simulation_start = start\n",
    "        self.simulation_end = end\n",
    "\n",
    "        # how many learning roles do exist and how are they named\n",
    "        self.buffer: ReplayBuffer = None\n",
    "        self.obs_dim = learning_config[\"observation_dimension\"]\n",
    "        self.act_dim = learning_config[\"action_dimension\"]\n",
    "        self.episodes_done = 0\n",
    "        self.rl_strats: dict[int, LearningStrategy] = {}\n",
    "        self.rl_algorithm = learning_config[\"algorithm\"]\n",
    "        self.critics = {}\n",
    "        self.target_critics = {}\n",
    "\n",
    "        # define whether we train model or evaluate it\n",
    "        self.training_episodes = learning_config[\"training_episodes\"]\n",
    "        self.learning_mode = learning_config[\"learning_mode\"]\n",
    "        self.continue_learning = learning_config[\"continue_learning\"]\n",
    "        self.trained_policies_save_path = learning_config[\"trained_policies_save_path\"]\n",
    "        self.trained_policies_load_path = learning_config.get(\n",
    "            \"trained_policies_load_path\", self.trained_policies_save_path\n",
    "        )\n",
    "\n",
    "        cuda_device = (\n",
    "            learning_config[\"device\"]\n",
    "            if \"cuda\" in learning_config.get(\"device\", \"cpu\")\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        self.device = th.device(cuda_device if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # future: add option to choose between float16 and float32\n",
    "        # float_type = learning_config.get(\"float_type\", \"float32\")\n",
    "        self.float_type = th.float\n",
    "\n",
    "        th.backends.cuda.matmul.allow_tf32 = True\n",
    "        th.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "        self.learning_rate = learning_config.get(\"learning_rate\", 1e-4)\n",
    "\n",
    "        # if we do not have initital experience collected we will get an error as no samples are avaiable on the\n",
    "        # buffer from which we can draw exprience to adapt the strategy, hence we set it to minium one episode\n",
    "\n",
    "        self.episodes_collecting_initial_experience = max(\n",
    "            learning_config.get(\"episodes_collecting_initial_experience\", 5), 1\n",
    "        )\n",
    "\n",
    "        self.train_freq = learning_config.get(\"train_freq\", 1)\n",
    "        self.gradient_steps = (\n",
    "            self.train_freq\n",
    "            if learning_config.get(\"gradient_steps\", -1) == -1\n",
    "            else learning_config[\"gradient_steps\"]\n",
    "        )\n",
    "        self.batch_size = learning_config.get(\"batch_size\", 128)\n",
    "        self.gamma = learning_config.get(\"gamma\", 0.99)\n",
    "\n",
    "        self.eval_episodes_done = 0\n",
    "\n",
    "        # function that initializes learning, needs to be an extra function so that it can be called after buffer is given to Role\n",
    "        self.create_learning_algorithm(self.rl_algorithm)\n",
    "\n",
    "        # store evaluation values\n",
    "        self.max_eval = defaultdict(lambda: -1e9)\n",
    "        self.rl_eval = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ww-L9fABnw3"
   },
   "outputs": [],
   "source": [
    "#magic to enable class definitions across colab cells\n",
    "%%add_to Learning\n",
    "\n",
    "def setup(self) -> None:\n",
    "        \"\"\"\n",
    "        Set up the learning role for reinforcement learning training.\n",
    "\n",
    "        Notes:\n",
    "            This method prepares the learning role for the reinforcement learning training process. It subscribes to relevant messages\n",
    "            for handling the training process and schedules recurrent tasks for policy updates based on the specified training frequency.\n",
    "        \"\"\"\n",
    "        # subscribe to messages for handling the training process\n",
    "        self.context.subscribe_message(\n",
    "            self,\n",
    "            self.handle_message,\n",
    "            lambda content, meta: content.get(\"context\") == \"rl_training\",\n",
    "        )\n",
    "\n",
    "        recurrency_task = rr.rrule(\n",
    "            freq=rr.HOURLY,\n",
    "            interval=self.train_freq,\n",
    "            dtstart=self.simulation_start,\n",
    "            until=self.simulation_end,\n",
    "            cache=True,\n",
    "        )\n",
    "\n",
    "        self.context.schedule_recurrent_task(self.update_policy, recurrency_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#magic to enable class definitions across colab cells\n",
    "%%add_to Learning\n",
    "\n",
    "\n",
    "def create_learning_algorithm(self, algorithm: RLAlgorithm):\n",
    "    \"\"\"\n",
    "    Create and initialize the reinforcement learning algorithm.\n",
    "\n",
    "    This method creates and initializes the reinforcement learning algorithm based on the specified algorithm name. The algorithm\n",
    "    is associated with the learning role and configured with relevant hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        algorithm (RLAlgorithm): The name of the reinforcement learning algorithm.\n",
    "    \"\"\"\n",
    "    if algorithm == \"matd3\":\n",
    "        self.rl_algorithm = TD3(\n",
    "            learning_role=self,\n",
    "            learning_rate=self.learning_rate,\n",
    "            episodes_collecting_initial_experience=self.episodes_collecting_initial_experience,\n",
    "            gradient_steps=self.gradient_steps,\n",
    "            batch_size=self.batch_size,\n",
    "            gamma=self.gamma,\n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"Learning algorithm {algorithm} not implemented!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to Learning\n",
    "\n",
    "def initialize_policy(self, actors_and_critics: dict = None) -> None:\n",
    "    \"\"\"\n",
    "    Initialize the policy of the reinforcement learning agent considering the respective algorithm.\n",
    "\n",
    "    This method initializes the policy (actor) of the reinforcement learning agent. It tests if we want to continue the learning process with\n",
    "    stored policies from a former training process. If so, it loads the policies from the specified directory. Otherwise, it initializes the\n",
    "    respective new policies.\n",
    "    \"\"\"\n",
    "\n",
    "    self.rl_algorithm.initialize_policy(actors_and_critics)\n",
    "\n",
    "    if self.continue_learning is True and actors_and_critics is None:\n",
    "        directory = self.trained_policies_load_path\n",
    "        if Path(directory).is_dir():\n",
    "            logger.info(f\"Loading pretrained policies from {directory}!\")\n",
    "            self.rl_algorithm.load_params(directory)\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"Folder with pretrained policies {directory} does not exist\"\n",
    "            )\n",
    "\n",
    "async def update_policy(self) -> None:\n",
    "    \"\"\"\n",
    "    Update the policy of the reinforcement learning agent.\n",
    "\n",
    "    This method is responsible for updating the policy (actor) of the reinforcement learning agent asynchronously. It checks if\n",
    "    the number of episodes completed is greater than the number of episodes required for initial experience collection. If so,\n",
    "    it triggers the policy update process by calling the `update_policy` method of the associated reinforcement learning algorithm.\n",
    "\n",
    "    Notes:\n",
    "        This method is typically scheduled to run periodically during training to continuously improve the agent's policy.\n",
    "    \"\"\"\n",
    "    if self.episodes_done > self.episodes_collecting_initial_experience:\n",
    "        self.rl_algorithm.update_policy()\n",
    "\n",
    "def compare_and_save_policies(self, metrics: dict) -> None:\n",
    "    \"\"\"\n",
    "    Compare evaluation metrics and save policies based on the best achieved performance according to the metrics calculated.\n",
    "\n",
    "    This method compares the evaluation metrics, such as reward, profit, and regret, and saves the policies if they achieve the\n",
    "    best performance in their respective categories. It iterates through the specified modes, compares the current evaluation\n",
    "    value with the previous best, and updates the best value if necessary. If an improvement is detected, it saves the policy\n",
    "    and associated parameters.\n",
    "\n",
    "    metrics contain a metric key like \"reward\" and the current value.\n",
    "    This function stores the policies with the highest metric.\n",
    "    So if minimize is required one should add for example \"minus_regret\" which is then maximized.\n",
    "\n",
    "    Notes:\n",
    "        This method is typically used during the evaluation phase to save policies that achieve superior performance.\n",
    "        Currently the best evaluation metric is still assessed by the development team and preliminary we use the average rewards.\n",
    "    \"\"\"\n",
    "    if not metrics:\n",
    "        logger.error(\"tried to save policies but did not get any metrics\")\n",
    "        return\n",
    "    # if the current values are a new max in one of the metrics - we store them in the default folder\n",
    "    first_has_new_max = False\n",
    "\n",
    "    # add current reward to list of all rewards\n",
    "    for metric, value in metrics.items():\n",
    "        self.rl_eval[metric].append(value)\n",
    "        if self.rl_eval[metric][-1] > self.max_eval[metric]:\n",
    "            self.max_eval[metric] = self.rl_eval[metric][-1]\n",
    "            if metric == list(metrics.keys())[0]:\n",
    "                first_has_new_max = True\n",
    "            # store the best for our current metric in its folder\n",
    "            self.rl_algorithm.save_params(\n",
    "                directory=f\"{self.trained_policies_save_path}/{metric}\"\n",
    "            )\n",
    "\n",
    "    # use last metric as default\n",
    "    if first_has_new_max:\n",
    "        self.rl_algorithm.save_params(directory=self.trained_policies_save_path)\n",
    "        logger.info(\n",
    "            f\"Policies saved, episode: {self.eval_episodes_done + 1}, {metric=}, value={value:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDYKZGERKJ6V"
   },
   "source": [
    "#### **Solution 1**\n",
    "\n",
    "First why do we scale?\n",
    "\n",
    "Scaling observations is a crucial preprocessing step in machine learning, including reinforcement learning. It involves transforming the features so that they all fall within a similar numerical range. This is important for several reasons. Firstly, it aids in numerical stability during training. Large input values can lead to numerical precision issues, potentially causing the algorithm to perform poorly or even fail to converge. By scaling the features, we mitigate this risk, ensuring a more stable and reliable learning process.\n",
    "\n",
    "Additionally, scaling promotes uniformity in the learning process. Many optimization algorithms, like gradient descent, adjust model parameters based on the magnitude of gradients. When features have vastly different scales, some may dominate the learning process, while others receive less attention. This imbalance can hinder convergence and result in a suboptimal model. Scaling addresses this issue, allowing the algorithm to treat all features equally and progress more efficiently towards an optimal solution. This not only expedites the learning process but also enhances the model's ability to generalize to new, unseen data. In essence, scaling observations is a fundamental practice that enhances the performance and robustness of machine learning models across a wide array of applications.\n",
    "\n",
    "According to this the scaling should ensure a similar range for all input parameteres. You can achieve that by chosing the following scaling factors. If you add new observations, choose your scaling factors wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "PYoI3ncSKJSX",
    "outputId": "4b4341d7-5a21-49c4-ee25-b8c55f693cd1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#scaling factors for all observations\n",
    "#residual load forecast\n",
    "scaling_factor_res_load = self.max_demand\n",
    "\n",
    "# price forecast\n",
    "scaling_factor_price = self.max_bid_price\n",
    "\n",
    "# total capacity\n",
    "scaling_factor_total_capacity = unit.max_power\n",
    "\n",
    "# marginal cost\n",
    "scaling_factor_marginal_cost = self.max_bid_price\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rW_1op6fCTV-"
   },
   "source": [
    "### 3.3 Choose an action\n",
    "\n",
    "To differentiate between the inflexible and flexible parts of a plant's generation capacity, we split the bids into two parts. The first bid part allows agents to bid a very low or even negative price for the inflexible capacity; this reflects the agent's motivation to stay infra-marginal during periods of very low net load (e.g., in periods of high solar and wind power generation) to avoid the cost of a shut-down and subsequent start-up of the plant. The flexible part of the capacity can be offered at a higher price to provide chances for higher profits. The actions of agent $i$ at time-step $t$ are defined as $a_{i,t} = [ep^\\mathrm{inflex}_{i,t}, ep^\\mathrm{flex}_{i,t}] \\in [ep^{min},ep^{max}]$, where $ep^\\mathrm{inflex}_{i,t}$ and $ep^\\mathrm{flex}_{i,t}$ are bid prices for the inflexible and flexible capacities, and $ep^{min},ep^{max}$ are minimal and maximal bid prices, respectively.\n",
    "\n",
    "How do we learn, how to make good decisions? Basically by try and error, also know as **exploration**. Exploration is a fundamental concept in reinforcement learning, representing the strategy by which an agent interacts with its environment to gather information about the consequences of its actions. This is crucial because without exploration, the agent might settle for suboptimal policies based on its initial knowledge, limiting its ability to discover more rewarding states or actions.\n",
    "\n",
    "In the initial stages of training, also often called initial exploration, it's imperative to employ almost random actions. This means having the agent take actions purely by chance. This seemingly counterintuitive approach serves a critical purpose. Initially, the agent lacks any meaningful information about the environment, making it impossible to make informed decisions. By taking random actions, it can quickly gather a broad range of experiences, allowing it to grasp the fundamental structure of the environment. These random actions serve as a kind of \"baseline exploration,\" providing a starting point from which the agent can refine its policy through learning. With our domain knowledge we can even guide the initial exploration process, to enhance learning capabilities.\n",
    "\n",
    "\n",
    "Following up on these concepts the following tasks will:\n",
    "1. obtain the action values from the neurnal net in the bidding staretgy and\n",
    "2. then transform theses values into the actual bids of an order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cho84Pqs2N2G"
   },
   "source": [
    "#### **Task 2.1**\n",
    "**Goal**: With the observations and noise we generate actions\n",
    "\n",
    "In the following task we define the actions for the initial exploration mode. As described before we can guide it by not letting it choose random actions but defining a base-bid on which we add a good amount of noise. In this way the initial strategy starts from a solution that we know works somewhat well. Define the respective base bid in the followin code. Remeber we are defining bids for a conventional power plant bidding in an Energy-Only-Market with a uniform pricing auction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ehlm5Z9CbRw"
   },
   "outputs": [],
   "source": [
    "#magic to enable class definitions across colab cells\n",
    "%%add_to RLStrategy\n",
    "def get_actions(self, next_observation):\n",
    "        \"\"\"\n",
    "        Get actions\n",
    "        \"\"\"\n",
    "\n",
    "        # distinction whetere we are in learning mode or not to handle exploration realised with noise\n",
    "        if self.learning_mode:\n",
    "            # if we are in learning mode the first x episodes we want to explore the entire action space\n",
    "            # to get a good initial experience, in the area around the costs of the agent\n",
    "            if self.collect_initial_experience_mode:\n",
    "                # define current action as soley noise\n",
    "                noise = (\n",
    "                    th.normal(\n",
    "                        mean=0.0, std=0.2, size=(1, self.act_dim), dtype=self.float_type\n",
    "                    )\n",
    "                    .to(self.device)\n",
    "                    .squeeze()\n",
    "                )\n",
    "\n",
    "                # =============================================================================\n",
    "                # 2.1 Get Actions and handle exploration\n",
    "                # =============================================================================\n",
    "                #==> YOUR CODE HERE\n",
    "                base_bid = #TODO\n",
    "\n",
    "                # add niose to the last dimension of the observation\n",
    "                # needs to be adjusted if observation space is changed, because only makes sense\n",
    "                # if the last dimension of the observation space are the marginal cost\n",
    "                curr_action = noise + base_bid.clone().detach()\n",
    "\n",
    "            else:\n",
    "                # if we are not in the initial exploration phase we chose the action with the actor neuronal net\n",
    "                # and add noise to the action\n",
    "                curr_action = self.actor(next_observation).detach()\n",
    "                noise = th.tensor(\n",
    "                    self.action_noise.noise(), device=self.device, dtype=self.float_type\n",
    "                )\n",
    "                curr_action += noise\n",
    "        else:\n",
    "            # if we are not in learning mode we just use the actor neuronal net to get the action without adding noise\n",
    "\n",
    "            curr_action = self.actor(next_observation).detach()\n",
    "            noise = tuple(0 for _ in range(self.act_dim))\n",
    "\n",
    "        curr_action = curr_action.clamp(-1, 1)\n",
    "\n",
    "        return curr_action, noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTaqkwV3xcf6"
   },
   "source": [
    "#### **Solution 2.1**\n",
    "\n",
    "So how do we define the base bid?\n",
    "\n",
    "Assuming the described auction is a efficient market with full information and competition, we know that bidding the marginal costs of the power plant is the economically best bid. With the RL strategy we can recreate the abuse of market power and incomplete information, which enables us to model different market settings. Yet, starting of with the theoretically styleized optimal solution guides our RL agents porperly. As the marginal costs of the power plant are part of the oberservations we can define the base bid in the following way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rfXJBGOKxbk7",
    "outputId": "06f76c52-e215-4998-8f61-f7492b880e4d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#base_bid = marginal costs\n",
    "base_bid = next_observation[-1] # = marginal_costs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5Hgh88Vz0wD"
   },
   "source": [
    "#### **Task 2.2**\n",
    "**Goal: Define the actual bids with the outputs of the actors**\n",
    "\n",
    "Similarly to every other output of a neuronal network, the actions are in the range of 0-1. These values need to be translated into the actual bids $a_{i,t} = [ep^\\mathrm{inflex}_{i,t}, ep^\\mathrm{flex}_{i,t}] \\in [ep^{min},ep^{max}]$. This can be done in a way that further helps the RL agent to learn, if we put some thought into.\n",
    "\n",
    "For this we go back into the calculate_bids() function and instead of just defining bids=actions, which was just a place holder, we actually make them into bids. Think about a smart way to transform them and fill the gaps in the following code. Remember:\n",
    "\n",
    "  - *bid_quantity_inflex* represent the inflexible part of the bid. This represents the minimum run capacity of the unit.\n",
    "  - *bid_quantity_flex* represent the flexible part of the bid. This represents the flexible capacity of the unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y81HzlkjNHJ0"
   },
   "outputs": [],
   "source": [
    "#magic to enable class definitions across colab cells\n",
    "%%add_to RLStrategy\n",
    "def calculate_bids(\n",
    "    self,\n",
    "    unit: SupportsMinMax,\n",
    "    market_config: MarketConfig,\n",
    "    product_tuples: list[Product],\n",
    "    **kwargs,\n",
    ") -> Orderbook:\n",
    "    \"\"\"\n",
    "    Calculate bids for a unit\n",
    "    \"\"\"\n",
    "\n",
    "    bid_quantity_inflex, bid_price_inflex = 0, 0\n",
    "    bid_quantity_flex, bid_price_flex = 0, 0\n",
    "\n",
    "    start = product_tuples[0][0]\n",
    "    end = product_tuples[0][1]\n",
    "    # get technical bounds for the unit output from the unit\n",
    "    min_power, max_power = unit.calculate_min_max_power(start, end)\n",
    "    min_power = min_power[start]\n",
    "    max_power = max_power[start]\n",
    "\n",
    "    # =============================================================================\n",
    "    # 1. Get the Observations, which are the basis of the action decision\n",
    "    # =============================================================================\n",
    "    next_observation = self.create_observation(\n",
    "        unit=unit,\n",
    "        market_id=market_config.market_id,\n",
    "        start=start,\n",
    "        end=end,\n",
    "    )\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2. Get the Actions, based on the observations\n",
    "    # =============================================================================\n",
    "    actions, noise = self.get_actions(next_observation)\n",
    "\n",
    "    bids = actions\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3.2 Transform Actions into bids\n",
    "    # =============================================================================\n",
    "    #==> YOUR CODE HERE\n",
    "    # actions are in the range [0,1], we need to transform them into actual bids\n",
    "    # we can use our domain knowledge to guide the bid formulation\n",
    "    bid_prices = actions * self.max_bid_price\n",
    "\n",
    "    # 3.1 formulate the bids for Pmin\n",
    "    # Pmin, the minium run capacity is the inflexible part of the bid, which should always be accepted\n",
    "    bid_quantity_inflex = min_power\n",
    "    bid_price_inflex = #TODO\n",
    "\n",
    "    # 3.1 formulate the bids for Pmax - Pmin\n",
    "    # Pmin, the minium run capacity is the inflexible part of the bid, which should always be accepted\n",
    "    bid_quantity_flex = max_power - bid_quantity_inflex\n",
    "    bid_price_flex = #TODO\n",
    "\n",
    "    # actually formulate bids in orderbook format\n",
    "    bids = [\n",
    "        {\n",
    "            \"start_time\": start,\n",
    "            \"end_time\": end,\n",
    "            \"only_hours\": None,\n",
    "            \"price\": bid_price_inflex,\n",
    "            \"volume\": bid_quantity_inflex,\n",
    "        },\n",
    "        {\n",
    "            \"start_time\": start,\n",
    "            \"end_time\": end,\n",
    "            \"only_hours\": None,\n",
    "            \"price\": bid_price_flex,\n",
    "            \"volume\": bid_quantity_flex,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # store results in unit outputs which are written to database by unit operator\n",
    "    unit.outputs[\"rl_observations\"][start] = next_observation\n",
    "    unit.outputs[\"rl_actions\"][start] = actions\n",
    "    unit.outputs[\"rl_exploration_noise\"][start] = noise\n",
    "    \n",
    "    bids = self.remove_empty_bids(bids)\n",
    "\n",
    "    return bids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n-kJeOFCfRB"
   },
   "source": [
    "#### **Solution 2.2**\n",
    "\n",
    "So how do we define the actual bid from the action?\n",
    "\n",
    "We have the bid price for the minimum power (inflex) and the rest of the power. As the power plant needs to run at minimal the minum power in order to offer generation in general, it makes sense to offer this generation at a lower price than the rest of the power. Hence, we can allocate the actions to the bid prices in the following way. In addition, the actions need to be rescaled of course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "wB7X-pFkCje3",
    "outputId": "ff905a9d-e3f2-4487-9e8a-9dbf4e855ab7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#calculate actual bids\n",
    "#rescale actions to actual prices\n",
    "bid_prices = actions * self.max_bid_price\n",
    "\n",
    "#calculate inflexible part of the bid\n",
    "bid_quantity_inflex = min_power\n",
    "bid_price_inflex = min(bid_prices)\n",
    "\n",
    "#calculate flexible part of the bid\n",
    "bid_quantity_flex = max_power - bid_quantity_inflex\n",
    "bid_price_flex = max(bid_prices)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr15xKuGCkbn"
   },
   "source": [
    "### 3.4 Get a reward\n",
    "This step is done in the *calculate_reward*()-function, which is called after the market is cleared and we get the market feedback, so we can calculate the profit. In RL, the design of a reward function is as important as the choice of the correct algorithm. During the initial phase of the work, pure economic reward in the form of the agent's profit was used. Typically, electricity market models consider only a single restart cost. Still, in the case of using RL, the split into shut-down and start-up costs allow the agents to better differentiate between these two events and learn a better policy.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_{i,t} =\n",
    "\\begin{cases}\n",
    "P^\\text{conf}_{i,t} (M_t - mc_{i,t}) dt - c^{su}_i & \\text{if $P^\\text{conf}_{i,t}$ $\\geq  P^{min}_i$} \\\\\n",
    "& \\text{and $P_{i,t-1}$ $= 0$} \\\\\n",
    "P^\\text{conf}_{i,t} (M_t - mc_{i,t}) dt & \\text{if $P^\\text{conf}_{i,t}$ $\\geq  P^{min}_i$} \\\\\n",
    "& \\text{and $P_{i,t-1}$ $\\neq 0$} \\\\\n",
    "- c^{sd}_i & \\text{if $P^\\text{conf}_{i,t}$ $\\leq  P^{min}_i$} \\\\\n",
    "& \\text{and $P_{i,t-1}$ $\\neq 0$} \\\\\n",
    "0 & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In this equation, the variables are:\n",
    "* $P^\\text{conf}$ the confirmed capacity on the market\n",
    "* $P^{min}$ the minimal stable capacity\n",
    "* $M$ the market clearing price\n",
    "* $mc$ the marginal generation cost\n",
    "* $dt$ the market time resolution\n",
    "* $c^{su}, c^{sd}$ the start-up and shut-down costs, respectively\n",
    "\n",
    "The profit-driven reward function was sufficient for a few agents, but the learning performance decreased significantly with more agents. Therefore, we add an additional regret term $cm$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGyaOUgo3Y8Q"
   },
   "source": [
    "#### **Task 3**\n",
    "**Goal**: Define the reward guiding the learning process of the agent.\n",
    "\n",
    "As the reward plays such a crucial role in the learning think of ways how to integrate further signals exceeding the monetary profit. One example could be integrating a regret term, namely the opportunity costs. Your task is to define the rewrad using the opportunity costs and to scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9HX41mODuBU"
   },
   "outputs": [],
   "source": [
    "#magic to enable class definitions across colab cells\n",
    "%%add_to RLStrategy\n",
    "def calculate_reward(\n",
    "        self,\n",
    "        unit,\n",
    "        marketconfig: MarketConfig,\n",
    "        orderbook: Orderbook,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculate reward\n",
    "    \"\"\"\n",
    "\n",
    "    # =============================================================================\n",
    "    # 3. Calculate Reward\n",
    "    # =============================================================================\n",
    "    # function is called after the market is cleared and we get the market feedback,\n",
    "    # so we can calculate the profit\n",
    "\n",
    "    product_type = marketconfig.product_type\n",
    "\n",
    "    profit = 0\n",
    "    reward = 0\n",
    "    opportunity_cost = 0\n",
    "\n",
    "    # iterate over all orders in the orderbook, to calculate order specific profit\n",
    "    for order in orderbook:\n",
    "        start = order[\"start_time\"]\n",
    "        end = order[\"end_time\"]\n",
    "        end_excl = end - unit.index.freq\n",
    "\n",
    "        # depending on way the unit calaculates marginal costs we take costs\n",
    "        if unit.marginal_cost is not None:\n",
    "            marginal_cost = (\n",
    "                unit.marginal_cost[start]\n",
    "                if len(unit.marginal_cost) > 1\n",
    "                else unit.marginal_cost\n",
    "            )\n",
    "        else:\n",
    "            marginal_cost = unit.calc_marginal_cost_with_partial_eff(\n",
    "                power_output=unit.outputs[product_type].loc[start:end_excl],\n",
    "                timestep=start,\n",
    "            )\n",
    "\n",
    "        duration = (end - start) / timedelta(hours=1)\n",
    "\n",
    "        # calculate profit as income - running_cost from this event\n",
    "        price_difference = order[\"accepted_price\"] - marginal_cost\n",
    "        order_profit = price_difference * order[\"accepted_volume\"] * duration\n",
    "\n",
    "        # calculate opportunity cost\n",
    "        # as the loss of income we have because we are not running at full power\n",
    "        order_opportunity_cost = (\n",
    "            price_difference\n",
    "            * (\n",
    "                unit.max_power - unit.outputs[product_type].loc[start:end_excl]\n",
    "            ).sum()\n",
    "            * duration\n",
    "        )\n",
    "\n",
    "        # if our opportunity costs are negative, we did not miss an opportunity to earn money and we set them to 0\n",
    "        order_opportunity_cost = max(order_opportunity_cost, 0)\n",
    "\n",
    "        # collect profit and opportunity cost for all orders\n",
    "        opportunity_cost += order_opportunity_cost\n",
    "        profit += order_profit\n",
    "\n",
    "    # consideration of start-up costs, which are evenly divided between the\n",
    "    # upward and downward regulation events\n",
    "    if (\n",
    "        unit.outputs[product_type].loc[start] != 0\n",
    "        and unit.outputs[product_type].loc[start - unit.index.freq] == 0\n",
    "    ):\n",
    "        profit = profit - unit.hot_start_cost / 2\n",
    "    elif (\n",
    "        unit.outputs[product_type].loc[start] == 0\n",
    "        and unit.outputs[product_type].loc[start - unit.index.freq] != 0\n",
    "    ):\n",
    "        profit = profit - unit.hot_start_cost / 2\n",
    "\n",
    "    # =============================================================================\n",
    "    # =============================================================================\n",
    "    # ==> YOUR CODE HERE\n",
    "    # The straight forward implemntation would be reward = profit, yet we would like to give the agent more guidance\n",
    "    # in the learning process, so we add a regret term to the reward, which is the opportunity cost\n",
    "    # define the reward and scale it\n",
    "\n",
    "    scaling = #TODO\n",
    "    regret_scale = #TODO\n",
    "    reward = #TODO\n",
    "\n",
    "    # store results in unit outputs which are written to database by unit operator\n",
    "    unit.outputs[\"profit\"].loc[start:end_excl] += profit\n",
    "    unit.outputs[\"reward\"].loc[start:end_excl] = reward\n",
    "    unit.outputs[\"regret\"].loc[start:end_excl] = opportunity_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWF7D4QA2-kz"
   },
   "source": [
    "#### **Solution 3**\n",
    "\n",
    "So how do we define the actual reward?\n",
    "\n",
    "We use the opportunity costs for further guidance, which quantify the expected contribution margin, as defined by the following equation, with $P^{max}$ as the maximal available capacity.\n",
    "\n",
    "\\begin{equation}\n",
    "cm_{i,t} = \\max[(P^{max}_i - P^\\text{conf}_{i,t}) (M_t - mc_{i,t}) dt, 0]\n",
    "\\end{equation}\n",
    "\n",
    "The regret term gives a negative signal to the agent when there is opportunity cost due to the unsold capacity, thus correcting the agent's actions. This term also introduces an increased influence of the competition between agents in learning. By minimizing the regret, the agents drive the bid prices closer to the marginal generation cost, which drives the market price down.\n",
    "\n",
    "The reward of agent $i$ at time-step $t$ is defined by the equation below.\n",
    "\n",
    "\\begin{equation}\n",
    "R_{i,t}  = \\pi_{i,t} + \\beta cm_{i,t}\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\beta$ is the regret scaling factor to adjust the ratio between profit-maximizing and regret-minimizing learning.\n",
    "\n",
    "The described reward function has proven to perform well even with many agents and to accelerate learning convergence. This is because minimizing the regret term drives the overall system to equilibrium. At a point close to the equilibrium point, the average reward of all agents would converge to a constant value since further policy changes would not lead to an additional reduction in regrets or an increase in profits. Therefore, the average reward value can also be a good indicator of learning performance and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "e1XdVXPSCo_k",
    "outputId": "585d94a5-7475-4e96-d0a1-5e82b711c6a5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "scaling = 0.1 / unit.max_power\n",
    "regret_scale = 0.2\n",
    "reward = float(profit - regret_scale * opportunity_cost) * scaling\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3flH5iY4x7Z"
   },
   "source": [
    "### 3.5 Start the simulation\n",
    "\n",
    "We are almost done with all the changes to actually be able to make ASSUME learn here in google colab. If you would rather like to load our pretrained strategies, we need a function for loading parameters, which can be found below.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwVtpK3B5gR6"
   },
   "outputs": [],
   "source": [
    "# magic to enable class definitions across colab cells\n",
    "%%add_to RLStrategy\n",
    "\n",
    "\n",
    "def load_actor_params(self, load_path):\n",
    "    \"\"\"\n",
    "    Load actor parameters\n",
    "    \"\"\"\n",
    "    directory = f\"{load_path}/actors/actor_{self.unit_id}.pt\"\n",
    "\n",
    "    params = th.load(directory, map_location=self.device)\n",
    "\n",
    "    self.actor = Actor(self.obs_dim, self.act_dim, self.float_type)\n",
    "    self.actor.load_state_dict(params[\"actor\"])\n",
    "\n",
    "    if self.learning_mode:\n",
    "        self.actor_target = Actor(self.obs_dim, self.act_dim, self.float_type)\n",
    "        self.actor_target.load_state_dict(params[\"actor_target\"])\n",
    "        self.actor_target.eval()\n",
    "        self.actor.optimizer.load_state_dict(params[\"actor_optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTlqMouufKyo"
   },
   "source": [
    "To control the learning process, the config file determines the parameters of the learning algorithm. As we want to temper with these values in the notebook we will overwrite the learning config in the next cell and then load it into our world.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moZ_UD7FfkOh"
   },
   "outputs": [],
   "source": [
    "learning_config = {\n",
    "    \"observation_dimension\": 50,\n",
    "    \"action_dimension\": 2,\n",
    "    \"continue_learning\": False,\n",
    "    \"trained_policies_save_path\": \"None\",\n",
    "    \"max_bid_price\": 100,\n",
    "    \"algorithm\": \"matd3\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_episodes\": 100,\n",
    "    \"episodes_collecting_initial_experience\": 5,\n",
    "    \"train_freq\": 24,\n",
    "    \"gradient_steps\": -1,\n",
    "    \"batch_size\": 256,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"noise_sigma\": 0.1,\n",
    "    \"noise_scale\": 1,\n",
    "    \"noise_dt\": 1,\n",
    "    \"validation_episodes_interval\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPz8v4N5hpfr"
   },
   "outputs": [],
   "source": [
    "# Read the YAML file\n",
    "with open(\"assume/examples/inputs/example_02a/config.yaml\", \"r\") as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "# store our modifications to the config file\n",
    "data[\"base\"][\"learning_mode\"] = True\n",
    "data[\"base\"][\"learning_config\"] = learning_config\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open(\"assume/examples/inputs/example_02a/config.yaml\", \"w\") as file:\n",
    "    yaml.safe_dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlRnTgCy5d9W"
   },
   "source": [
    "In order to let the simulation run with the integrated learning we need to touch up the main file that runs it in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZlWxXxZr54WV",
    "outputId": "e30f4279-7a4e-4efc-9cfb-61416e4fe2f1"
   },
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "\n",
    "csv_path = \"./outputs\"\n",
    "os.makedirs(\"./local_db\", exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Available examples:\n",
    "    - local_db: without database and grafana\n",
    "    - timescale: with database and grafana (note: you need docker installed)\n",
    "    \"\"\"\n",
    "    data_format = \"local_db\"  # \"local_db\" or \"timescale\"\n",
    "\n",
    "    if data_format == \"local_db\":\n",
    "        db_uri = \"sqlite:///./local_db/assume_db.db\"\n",
    "    elif data_format == \"timescale\":\n",
    "        db_uri = \"postgresql://assume:assume@localhost:5432/assume\"\n",
    "\n",
    "    input_path = \"assume/examples/inputs\"\n",
    "    scenario = \"example_02a\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # create world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # we import our defined bidding strategey class including the learning into the world bidding strategies\n",
    "    # in the example files we provided the name of the learning bidding strategeis in the input csv is  \"pp_learning\"\n",
    "    # hence we define this strategey to be one of the learning class\n",
    "    world.bidding_strategies[\"pp_learning\"] = RLStrategy\n",
    "\n",
    "    # then we load the scenario specified above from the respective input files\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=input_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # run learning if learning mode is enabled\n",
    "    # needed as we simulate the modelling horizon multiple times to train reinforcement learning run_learning( world, inputs_path=input_path, scenario=scenario, study_case=study_case, )\n",
    "\n",
    "    if world.learning_config.get(\"learning_mode\", False):\n",
    "        run_learning(\n",
    "            world,\n",
    "            inputs_path=input_path,\n",
    "            scenario=scenario,\n",
    "            study_case=study_case,\n",
    "        )\n",
    "\n",
    "    # after the learning is done we make a normal run of the simulation, which equasl a test run\n",
    "    world.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
